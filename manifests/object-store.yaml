# This is one option for deploying an S3-compatible Object Store.
# Ideally we'd re-use the one already running in a kURL deployment, but that's not yet feasible.
#
# CD4PE setup:
#   S3 Bucket Name: cd4pe
#   S3 Endpoint: https://minio:9000
#   AWS Access Key: minio
#   AWS Secret Key: `kubectl get secret minio -o jsonpath="{.data.secretKey}" | base64 --decode`
# coredns manually configured to direct cd4pe.minio to the service IP address
---
apiVersion: v1
kind: Secret
metadata:
  name: minio
  namespace: '{{repl or (ConfigOption "namespace") Namespace }}'
type: Opaque
stringData:
  accessKey: '{{repl ConfigOption "minio_access_key"}}'
  secretKey: '{{repl ConfigOption "minio_secret_key"}}'
---
apiVersion: v1
kind: Service
metadata:
  name: minio
  namespace: '{{repl or (ConfigOption "namespace") Namespace}}'
  labels:
    app: minio
spec:
  publishNotReadyAddresses: true
  clusterIP: None
  ports:
    - port: 9000
      name: minio
  selector:
    app: minio
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: minio
  namespace: '{{repl or (ConfigOption "namespace") Namespace}}'
spec:
  serviceName: minio
  podManagementPolicy: Parallel
  replicas: 4
  selector:
    matchLabels:
      app: minio # has to match .spec.template.metadata.labels
  template:
    metadata:
      labels:
        app: minio # has to match .spec.selector.matchLabels
    spec:
      containers:
      - name: minio
        env:
        - name: MINIO_DOMAIN
          value: minio
        - name: MINIO_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: minio
              key: accessKey
        - name: MINIO_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: minio
              key: secretKey
        image: minio/minio:RELEASE.2020-04-02T21-34-49Z
        args:
        - server
        - http://minio-{0...3}.minio.repl{{or (ConfigOption "namespace") Namespace}}.svc.cluster.local/data
        ports:
        - containerPort: 9000
        # These volume mounts are persistent. Each pod in the PetSet
        # gets a volume mounted based on this field.
        volumeMounts:
        - name: data
          mountPath: /data
        # Liveness probe detects situations where MinIO server instance
        # is not working properly and needs restart. Kubernetes automatically
        # restarts the pods if liveness checks fail.
        livenessProbe:
          httpGet:
            path: /minio/health/live
            port: 9000
          initialDelaySeconds: 5
          periodSeconds: 30
          timeoutSeconds: 1
          successThreshold: 1
          failureThreshold: 3
        # Readiness probe detects situations where MinIO server instance
        # is not ready to accept connections. Kubernetes automatically
        # stops all the traffic to the pods if readiness checks fail.
        readinessProbe:
          httpGet:
            path: /minio/health/ready
            port: 9000
          initialDelaySeconds: 60
          periodSeconds: 15
          timeoutSeconds: 1
          successThreshold: 1
          failureThreshold: 3
  # These are converted to volume claims by the controller
  # and mounted at the paths mentioned above.
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 2Gi
---
apiVersion: v1
kind: Service
metadata:
  name: minio-service
  namespace: '{{repl or (ConfigOption "namespace") Namespace}}'
spec:
  ports:
    - port: 9000
      protocol: TCP
  selector:
    app: minio
